{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1336523a",
   "metadata": {},
   "source": [
    "# Generalized Lotka Volterra: joint alternate estimation for inverse problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca2615a-b8a8-4473-a6b3-5dbe9a774260",
   "metadata": {},
   "source": [
    "*Hugo Gangloff$^1$, Nicolas Jouvin$^1$, Lorenzo Sala$^2$ - December 2025*\n",
    "\n",
    "$^1$ Université Paris-Saclay, AgroParisTech, INRAE UMR MIA Paris-Saclay, France\n",
    "\n",
    "$^2$ Université Paris-Saclay, INRAE, MaIAGE, 78350, Jouy-en-Josas, France "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e899d80-8387-45fa-8e60-0aeb3ce5cc82",
   "metadata": {},
   "source": [
    "**Abstract: In this notebook we jointly estimate the equation solution with a PINN and the equation parameter $\\theta$**. We will also **make use of available observations** and we will also **differentiate the dynamic loss with respect to the equation parameter $\\theta$**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cf796c-f5ca-439c-a0dd-2cd2213bd723",
   "metadata": {},
   "source": [
    "**Outline**\n",
    "\n",
    "1. [Mathematical background](#Mathematical-background)\n",
    "2. [Code set-up](#Code-set-up)\n",
    "3. [Joint alternate training (simple)](#Joint-alternate-training-(simple))\n",
    "4. [Joint alternate training (regularized)](#Joint-alternate-training-(regularized))\n",
    "5. [Vanilla joint training](#Vanilla-joint-training)\n",
    "6. [Vanilla joint training (regularized)](#Vanilla-joint-training-(regularized))\n",
    "7. [Comparison of the approaches](#Comparison-of-the-approaches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59366717-4c2b-415c-9392-49e6e3098ee3",
   "metadata": {},
   "source": [
    "## Mathematical background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5b8504-23b8-4d00-a111-fd4d72ca3e24",
   "metadata": {},
   "source": [
    "On the time interval $I=[0, T]$, we consider a **Generalized Lotka Volterra** system with $N_s$ populations, $\\forall t\\in I$, where $x_i(t)$ denotes the abundance (biomass) of species $i$ at time $t$:\n",
    "\n",
    "$$\n",
    "\\frac{dx_i(t)}{dt} = \\mu_i x_i(t) + \\sum_{j=1}^{N_s} a_{ij} x_i(t) x_j(t), \n",
    "\\quad i = 1, \\dots, N_s,\n",
    "$$\n",
    "\n",
    "where $\\mu_i$ are the growth rates and $a_{ij}$ are the interactions terms. We also have some initial conditions $x_i(0) = x_{i,0} > 0$.\n",
    "\n",
    "We will work with the GLV equation in logarithmic (for stability and positiveness) and vectorial form (for concision):\n",
    "\n",
    "$$\n",
    "\\frac{du(t)}{dt} = \\mu + A \\exp(u(t)),\n",
    "$$\n",
    "where $u(t) = (\\log x_1(t), \\dots, \\log x_{N_s}(t))^\\top$, \n",
    "$\\mu = (\\mu_1, \\dots, \\mu_{N_s})^\\top$, \n",
    "and $A = (a_{ij})_{1 \\le i,j \\le N_s}$. Therefore $\\theta=\\{\\mu, A\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5292d3ca",
   "metadata": {},
   "source": [
    "We are working the context of **inverse problem**, we also have at our disposal **noisy observations of the solution** at some sampled times $\\{t_j, u_j\\}_{j=1}^{n_o}$ that we will use to estimate both the solution $u$ and the parameters $\\theta$. We work the in the **PINN methodology**, we approximate the solution as a neural network $u_\\nu : I \\mapsto \\mathbb{R}^{N_s}$, where $\\nu$ are the weights and biases of the neural network that we need to estimate.\n",
    "\n",
    "\n",
    "Therefore, we aim at solving:\n",
    "\n",
    "$$\n",
    "\\hat{\\nu},\\hat{\\theta}=\\arg\\min_{\\nu,\\theta} \\mathcal{L}(\\nu,\\theta)\n",
    "$$\n",
    "with the loss:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(\\nu,\\theta)&=\\frac{1}{n_c} \\sum_{i=1}^{n_c} \\Vert \\frac{du_\\nu(t_i)}{dt} - \\mu - A \\exp(u_\\nu(t_i))\\Vert^2 + \\Vert u_\\nu(0) - u_0 \\Vert_2^2\\\\\n",
    "&+\\frac{1}{n_o} \\sum_{j=1}^{n_o} \\Vert u_\\nu(t_j) - u_j \\Vert^2,\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\{t_i\\}_{i=1}^{i=n_c}$ is a set of randomly selected collocation points and $u_0=(\\log x_{1, 0}, \\dots, \\log x_{N_s,0})$ is the vector of initial condition values.\n",
    "\n",
    "More precisely, [Sections 3](#Joint-alternate-training-(simple)) and [Section 4](#Joint-alternate-training-(regularized)) will consider a slightly modified version of the problem above, namely, an alternate optimization between $\\nu$ and $\\theta=\\{\\mu,A\\}$ will be set up.\n",
    "\n",
    "- The optimization scheme for step $n+1$ can then be written for [Section 3](#Joint-alternate-training-(simple)):\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "        \\{\\mu,A\\}^{[n+1]} & \\leftarrow \\arg\\min_{\\{\\mu,A\\}} \\frac{1}{n_c} \\sum_{i=1}^{n_c} \\Vert \\frac{du_{\\nu^{[n]}}(t_i)}{dt} - \\mu - A \\exp(u_{\\nu^{[n]}}(t_i))\\Vert^2 \\\\\n",
    "        \\nu^{[n+1]} & \\leftarrow \\arg\\min_{\\nu} \\mathcal{L}(\\nu,\\{\\mu,A\\}^{[n+1]}).\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- The optimization scheme for step $n+1$ can then be written for [Section 4](#Joint-alternate-training-(regularized)):\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "        \\{\\mu,A\\}^{[n+1]} & \\leftarrow \\arg\\min_{\\{\\mu,A\\}} \\frac{1}{n_c} \\sum_{i=1}^{n_c} \\Vert \\frac{du_{\\nu^{[n]}}(t_i)}{dt} - \\mu - A \\exp(u_{\\nu^{[n]}}(t_i))\\Vert^2 + \\lambda_\\theta \\Vert \\theta \\Vert_1  \\\\\n",
    "        \\nu^{[n+1]} & \\leftarrow \\arg\\min_{\\nu} \\mathcal{L}(\\nu,\\{\\mu,A\\}^{[n+1]}) + \\lambda_\\nu \\Vert\\nu\\Vert^2_2,\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where we added a lasso penalization on the coefficients $\\theta$ and a ridge regularization on $\\nu$.\n",
    "\n",
    "On the other hand, [Section 5](#Vanilla-joint-training) and [Section 6](#Vanilla-joint-training-(regularized)) will perform a direct joint optimization on both set of parameters.\n",
    "\n",
    "- The optimization problem can then be written for [Section 5](#Vanilla-joint-training):\n",
    "\n",
    "$$\n",
    "\\hat{\\nu},\\hat{\\theta} = \\arg\\min_{\\nu,\\theta} \\mathcal{L}(\\nu,\\theta).\n",
    "$$\n",
    "\n",
    "- The optimization problem can then be written for [Section 6](#Vanilla-joint-training-(regularized)):\n",
    "\n",
    "$$\n",
    "\\hat{\\nu},\\hat{\\theta} = \\arg\\min_{\\nu,\\theta} \\mathcal{L}(\\nu,\\theta) + \\lambda_\\nu \\Vert\\nu\\Vert^2_2  + \\lambda_\\theta \\Vert \\theta \\Vert_1.\n",
    "$$\n",
    "\n",
    "**Initialization**: in all cases:\n",
    "\n",
    "- $\\theta$ is initialization to the $0$ of its vector space.\n",
    "\n",
    "- $\\nu$ parameters are first initialized with `equinox` default initialization for weight and biaises in linear layers. However a **pretraining on data only** is performed for all optimization problems. That is the joint optimizations all begin with a $\\nu^{ini}$ such that:\n",
    "\n",
    "$$\n",
    "\\nu^{ini} = \\arg\\min_{\\nu} \\frac{1}{n_o} \\sum_{j=1}^{n_o} \\Vert u_\\nu(t_j) - u_j \\Vert^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6b0451-76db-4eac-9c6d-d595b7b4fd01",
   "metadata": {},
   "source": [
    "## Code set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf8bebc-b311-4eb4-ad63-11447f62b280",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d13b5b",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d1c64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jinns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca7fe80",
   "metadata": {},
   "source": [
    "Import other dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abe5254-7556-424e-a57e-d364d67244a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "\n",
    "# from jax import config\n",
    "# config.update(\"jax_enable_x64\", True)\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import equinox as eqx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "figsize = (4, 4)\n",
    "plt.rcParams[\"figure.figsize\"] = figsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8701bb78-bac7-4556-9998-324dcb8d68b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbc6b36-341f-4c69-8c15-8c8a7b65e46d",
   "metadata": {},
   "source": [
    "### Define the equation parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e0761a-886f-405b-bb9b-60888cb1a76f",
   "metadata": {},
   "source": [
    "Time domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3cf23f-350f-489d-bcc1-671a9e4f987f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmin = 0\n",
    "tmax = 1\n",
    "\n",
    "Tmax = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e581bc50-0b4b-4ba1-88aa-871c2a996109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial conditions for each species\n",
    "\n",
    "### 3 population example\n",
    "# Ns = 3\n",
    "# N_0 = onp.array([3.0, 1.0, 4.0])\n",
    "# # growth rates for each species\n",
    "# growth_rates = jnp.array([0.01, 0.05, 0.04])\n",
    "\n",
    "# # interactions\n",
    "# interactions = -jnp.array([[0, 0.2, 0.1], [0.1, 0, 0.1], [0.1, 0.2, 0]])\n",
    "\n",
    "### 10 population example\n",
    "\n",
    "key = jax.random.PRNGKey(2)\n",
    "key, subkey = jax.random.split(key)\n",
    "\n",
    "Ns = 10\n",
    "A = jax.random.normal(subkey, shape=(Ns, Ns)) * 1e-2\n",
    "key, subkey = jax.random.split(key)\n",
    "dA = jnp.diag(A)\n",
    "key, subkey = jax.random.split(key)\n",
    "A *= jax.random.bernoulli(subkey, p=0.4, shape=A.shape)\n",
    "A = A - A.T\n",
    "A = A + jnp.diag(-jnp.abs(dA))\n",
    "interactions = A\n",
    "key, subkey = jax.random.split(key)\n",
    "growth_rates = jnp.abs(jax.random.normal(subkey, shape=(Ns,))) * 1e-1\n",
    "key, subkey = jax.random.split(key)\n",
    "N_0 = jax.random.uniform(subkey, shape=(Ns,)) * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae23114-1f82-4304-8e59-38370a9779f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "key, subkey = jax.random.split(key, 2)\n",
    "val_data = jinns.data.DataGeneratorODE(\n",
    "    key=subkey,\n",
    "    nt=500,\n",
    "    tmin=tmin,\n",
    "    tmax=tmax,\n",
    "    method=\"uniform\",\n",
    "    temporal_batch_size=None,\n",
    ")\n",
    "\n",
    "ts = val_data.times.sort(axis=0).squeeze()\n",
    "# NOTE parfois np.any(np.diff(ts) <= 0) qui provoque une erreur dans solve_ivp\n",
    "# mais on a toujours np.any(np.diff(ts) < 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab026f5c",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "We use observations obtained from a run of a ground truth solver/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1004a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.integrate import solve_ivp\n",
    "\n",
    "\n",
    "def GLV_RHS(t, y, theta):\n",
    "    \"\"\"\n",
    "    This equation is given in log space. That means the input y is the logarithm of the population density\n",
    "    \"\"\"\n",
    "    alpha, gamma = theta\n",
    "    return (alpha + (gamma @ jnp.exp(y).reshape((gamma.shape[0], 1))).squeeze()).ravel()\n",
    "\n",
    "\n",
    "# Define name bacteria\n",
    "names = [\"N1\", \"N2\", \"N3\"]\n",
    "N = len(names)\n",
    "\n",
    "# Define model parameters\n",
    "eq_params = (growth_rates, interactions)\n",
    "\n",
    "# Define initial bacterial populations\n",
    "y0 = N_0\n",
    "\n",
    "# Define time points\n",
    "t = ts * Tmax\n",
    "\n",
    "############################\n",
    "\n",
    "y0_log = np.log(y0)\n",
    "sol_log = solve_ivp(\n",
    "    fun=GLV_RHS,\n",
    "    y0=y0_log,\n",
    "    t_span=(0, Tmax),\n",
    "    t_eval=ts * Tmax,\n",
    "    args=(eq_params,),\n",
    ")\n",
    "y = np.exp(sol_log.y).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08ef367-d26c-44f1-a61a-218952d65bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data.times.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af685596-26e8-4cc5-85c2-b03457af579f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.any(np.diff(ts) <= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41c2f6e-2e2e-4314-8461-0a3363deecd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 500\n",
    "subsample_size = 10\n",
    "subsample = jnp.arange(n)\n",
    "subsample = subsample[:: int(n / (subsample_size - 1))]\n",
    "subsample = jnp.append(subsample[:-1], n - 1)\n",
    "t_subsample = ts[subsample]\n",
    "log_obs_subsample = jnp.log(y[subsample])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa63134",
   "metadata": {},
   "source": [
    "__Optionally blur the observed values__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11df636f-51d6-4ff6-92e3-6e1f35dcad63",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_level = 0.2\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "noise = noise_level * jax.random.normal(subkey, shape=log_obs_subsample.shape)\n",
    "log_obs_subsample_noisy = log_obs_subsample + noise\n",
    "# obs_subsample_noisy = jnp.where(obs_subsample_noisy < 1e-6, 1e-6, obs_subsample_noisy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4682994b-b688-4c59-8001-dece097db15a",
   "metadata": {},
   "source": [
    "**Visualize the noisy observations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addc5d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ts * Tmax, y)\n",
    "plt.gca().set_prop_cycle(None)\n",
    "plt.plot(t_subsample * Tmax, jnp.exp(log_obs_subsample_noisy), \"x\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e8d895-791c-416c-8cc1-b9c4d61d198f",
   "metadata": {},
   "source": [
    "### Estimate the solution and the parameters of the GLV system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcba23a-0aa6-4ff4-bac9-d3ffba022517",
   "metadata": {},
   "source": [
    "*Define the domain and the associated data generator*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2176b612-a389-4ac4-b2cd-3962f2de0ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "key, subkey = jax.random.split(key)\n",
    "train_data = jinns.data.DataGeneratorODE(\n",
    "    key=subkey,\n",
    "    nt=5000,\n",
    "    tmin=tmin,\n",
    "    tmax=tmax,\n",
    "    temporal_batch_size=None,\n",
    "    method=\"uniform\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d61b5d0-9d37-44bb-9107-caf9284eb850",
   "metadata": {},
   "source": [
    "*Define the data generator for the observations*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac900fd-708e-494c-9969-d32cad7e2d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "key, subkey = jax.random.split(key)\n",
    "obs_data = jinns.data.DataGeneratorObservations(\n",
    "    key=subkey,\n",
    "    obs_batch_size=None,\n",
    "    observed_pinn_in=t_subsample,\n",
    "    observed_values=log_obs_subsample_noisy,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8eaa1f-0c08-4f47-91b9-c0898aa33929",
   "metadata": {},
   "source": [
    "*Define the PINN*\n",
    "\n",
    "Create the neural network architecture for the PINN with `equinox`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d762640-1ba7-460b-955c-31b0b01400db",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_neurons = 7 * Ns\n",
    "eqx_list = (\n",
    "    (eqx.nn.Linear, 1, Ns),\n",
    "    (jax.nn.tanh,),\n",
    "    (eqx.nn.Linear, Ns, N_neurons),\n",
    "    (jax.nn.tanh,),\n",
    "    (eqx.nn.Linear, N_neurons, N_neurons),\n",
    "    (jax.nn.tanh,),\n",
    "    (eqx.nn.Linear, N_neurons, N_neurons),\n",
    "    (jax.nn.tanh,),\n",
    "    (eqx.nn.Linear, N_neurons, N_neurons),\n",
    "    (jax.nn.tanh,),\n",
    "    (eqx.nn.Linear, N_neurons, N_neurons),\n",
    "    (jax.nn.tanh,),\n",
    "    (eqx.nn.Linear, N_neurons, Ns),\n",
    "    (jax.nn.tanh,),\n",
    "    (eqx.nn.Linear, Ns, Ns),\n",
    ")\n",
    "key, subkey = jax.random.split(key)\n",
    "u, init_nn_params = jinns.nn.PINN_MLP.create(\n",
    "    key=subkey, eqx_list=eqx_list, eq_type=\"ODE\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eacdca3-2db1-4960-83db-21d3aad8fdba",
   "metadata": {},
   "source": [
    "*Define the jinns parameter object. **Equation parameters are initialized to 0 everywhere.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4190ac89-3667-4340-9cc2-734e0ca85538",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_params = jinns.parameters.Params(\n",
    "    nn_params=init_nn_params,\n",
    "    eq_params={\n",
    "        \"g\": jnp.zeros_like(growth_rates),\n",
    "        \"i\": jnp.zeros_like(interactions),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b96440-60c7-4318-9254-f943b6e81535",
   "metadata": {},
   "source": [
    "*Define the loss weights*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9396d007-04f1-4893-a3c8-c58c36845ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_weights = jinns.loss.LossWeightsODE(\n",
    "    dyn_loss=1.0, initial_condition=Tmax, observations=Tmax\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9481837d-f9e3-4ae4-b651-71dc178c4c2e",
   "metadata": {},
   "source": [
    "*Define the dynamic loss term*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51e50dd-51f2-4526-96fd-afea7d2dac15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLV_loss(jinns.loss.ODE):\n",
    "    def equation(self, t, u, params):\n",
    "        \"\"\"\n",
    "        This equation is given in log, this means that u is the log of the population density\n",
    "        \"\"\"\n",
    "        udt = jax.jacfwd(lambda x: u(x, params))(t).squeeze()\n",
    "        f = udt - self.Tmax * GLV_RHS(\n",
    "            t, u(t, params), (params.eq_params.g, params.eq_params.i)\n",
    "        )\n",
    "        return f\n",
    "\n",
    "\n",
    "dynamic_loss = GLV_loss(Tmax=Tmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0cdd49-2bbd-4583-875e-8111392e7fbe",
   "metadata": {},
   "source": [
    "*Define how to differentiate each term of the loss* Note that the main `params` of the problem must be passed since we use the `from_str()` constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df34165-e245-4360-8fd1-358640b1ded1",
   "metadata": {},
   "outputs": [],
   "source": [
    "derivative_keys = jinns.parameters.DerivativeKeysODE.from_str(\n",
    "    dyn_loss=jinns.parameters.Params(\n",
    "        nn_params=True,\n",
    "        eq_params={\n",
    "            \"g\": jax.tree.map(lambda _: True, init_params.eq_params.g),\n",
    "            \"i\": jax.tree.map(lambda _: True, init_params.eq_params.i),\n",
    "        },\n",
    "    ),\n",
    "    initial_condition=\"nn_params\",\n",
    "    observations=\"nn_params\",\n",
    "    params=init_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d27791d-f3d9-4c33-bc65-bb1cfd005c0f",
   "metadata": {},
   "source": [
    "*Define the ODE PINN losses*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7a4ba0-92cd-4e24-95f3-0b722d575c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = jinns.loss.LossODE(\n",
    "    u=u,\n",
    "    loss_weights=loss_weights,\n",
    "    dynamic_loss=dynamic_loss,\n",
    "    initial_condition=(float(tmin), jnp.log(y0)),\n",
    "    derivative_keys=derivative_keys,\n",
    "    params=init_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1535654b-1cfb-4f0b-a5d2-fbcd5431d008",
   "metadata": {},
   "source": [
    "*We will start with a PINN pretraining on observations only. One argument in favor of such pretraining is  to avoid $\\theta$ collapsing to $0$. All approaches that will follow will benefit from this pretraining.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a7f5d4-4de3-4083-8a52-e4fe2e769a08",
   "metadata": {},
   "source": [
    "**Pretraining**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d629a46a-798f-4f16-a31e-6f2a627bf8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_loss = jinns.loss.LossODE(\n",
    "    u=u,\n",
    "    loss_weights=jinns.loss.LossWeightsODE(\n",
    "        dyn_loss=0.0,\n",
    "        initial_condition=1.0,\n",
    "        observations=1.0,  # ZERO to block gradients from dynamic loss\n",
    "    ),\n",
    "    dynamic_loss=dynamic_loss,\n",
    "    initial_condition=(float(tmin), jnp.log(y0)),\n",
    "    derivative_keys=derivative_keys,\n",
    "    params=init_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19181b8-d840-45ee-ad6b-f97785a6d75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_tx = optax.adam(learning_rate=1e-3)\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "print(\"PINN pretraining with observations only\")\n",
    "(\n",
    "    pretrain_params,\n",
    "    total_loss_values,\n",
    "    loss_by_term_dict,\n",
    "    _,\n",
    "    _,\n",
    "    _,\n",
    "    _,\n",
    "    stored_loss_weight_terms,\n",
    "    _,\n",
    "    _,\n",
    "    _,\n",
    "    _,\n",
    ") = jinns.solve(\n",
    "    n_iter=10000,\n",
    "    init_params=init_params,\n",
    "    data=train_data,\n",
    "    optimizer=pretrain_tx,\n",
    "    loss=pretrain_loss,\n",
    "    obs_data=obs_data,\n",
    "    print_loss_every=1000,\n",
    "    key=subkey,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12e0276-3fb2-4e2f-915e-807e360358c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "\n",
    "pretrain_u_est = jax.vmap(lambda t_x: jnp.exp(u(t_x, pretrain_params)))\n",
    "\n",
    "plt.plot(ts * Tmax, y, linestyle=\"--\", label=\"True\")\n",
    "plt.gca().set_prop_cycle(None)\n",
    "plt.plot(ts * Tmax, pretrain_u_est(ts), label=\"PINN estimated\")\n",
    "# plt.plot(t_subsample * Tmax, jnp.exp(log_obs_subsample_noisy), 'x')\n",
    "plt.title(\"u(t,x) after pretraining\")\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], color=\"k\", linestyle=\"--\", label=\"True\"),\n",
    "    Line2D([0], [0], color=\"k\", linestyle=\"dotted\", label=\"Reconstructed\"),\n",
    "    Line2D([0], [0], color=\"k\", label=\"PINN estimated\"),\n",
    "]\n",
    "# Create the figure\n",
    "plt.legend(handles=legend_elements, loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f2e47b-26b5-4d23-b98f-3ad0d9e4a3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert jnp.allclose(\n",
    "    total_loss_values,\n",
    "    loss_by_term_dict.observations * loss_weights.observations\n",
    "    + loss_by_term_dict.initial_condition * loss_weights.initial_condition,\n",
    ")  # check that we only use observations and initial conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe72b24d-4578-4244-a40e-92beac84ff3f",
   "metadata": {},
   "source": [
    "**The next cell defines the number of iterations and is common to all the methods, with a slight adaptation for the non alternate methods**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa02f2b1-4413-479c-9b8d-c8783827aa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of alternate iterations\n",
    "n_iter = 200\n",
    "# number of iterations for each solver\n",
    "n_iter_by_solver = jinns.parameters.Params(\n",
    "    nn_params=250,\n",
    "    eq_params={\"g\": 250, \"i\": 250},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d25784e-cb54-4572-bfeb-8447103539ce",
   "metadata": {},
   "source": [
    "### Joint alternate training (simple)\n",
    "\n",
    "**with `jinns.solve_alternate()`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e77c6db-88d1-4cb8-beae-c67dc9257fe3",
   "metadata": {},
   "source": [
    "*We will need to construct the object needed for the `jinns.solve_alternate()` call.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96507771-c55a-463f-92d0-5f1296b81356",
   "metadata": {},
   "source": [
    "**Alternate joint estimation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64879fb9-6661-47b5-971a-26832b7156e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from utils_GLV_JointEstimation_Alternate import update_and_project\n",
    "\n",
    "update_and_project_g = partial(update_and_project, param=\"g\")\n",
    "update_and_project_i = partial(update_and_project, param=\"i\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596b0515-0210-4bed-a3ce-ea30c6526ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_learning_rate = 1e-3\n",
    "\n",
    "\n",
    "# solver for each set of parameters\n",
    "optimizers = jinns.parameters.Params(\n",
    "    nn_params=optax.adamw(learning_rate=start_learning_rate),\n",
    "    eq_params={\n",
    "        \"g\": optax.chain(\n",
    "            optax.adam(learning_rate=start_learning_rate),\n",
    "            update_and_project_g(),\n",
    "        ),\n",
    "        \"i\": optax.chain(\n",
    "            optax.adam(learning_rate=1e-6),\n",
    "            update_and_project_i(),\n",
    "        ),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3789ad7a-656f-45b1-8fe8-3a5be5b25325",
   "metadata": {},
   "source": [
    "*We now start the alternate training*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d83391-02e4-4088-9360-25b4455f2512",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    params_alternate,\n",
    "    total_loss_values,\n",
    "    loss_by_term_dict,\n",
    "    _,\n",
    "    _,\n",
    "    _,\n",
    "    stored_params,\n",
    "    stored_loss_weight_terms,\n",
    "    _,\n",
    "    _,\n",
    ") = jinns.solve_alternate(\n",
    "    n_iter=n_iter,\n",
    "    n_iter_by_solver=n_iter_by_solver,\n",
    "    init_params=pretrain_params,\n",
    "    data=train_data,\n",
    "    loss=loss,  # take the complete loss!\n",
    "    optimizers=optimizers,\n",
    "    verbose=True,\n",
    "    obs_data=obs_data,\n",
    "    key=subkey,\n",
    "    print_loss_every=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1281bbd7",
   "metadata": {},
   "source": [
    "**Results**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50941bc8",
   "metadata": {},
   "source": [
    "*Plot the loss values*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd778b4-d9d9-4f69-ad02-2a3f7eacf59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for loss_name, loss_values in loss_by_term_dict.items():\n",
    "    # if loss_name == \"observations\":\n",
    "    plt.plot(jnp.log10(loss_values), label=loss_name)\n",
    "plt.plot(jnp.log10(total_loss_values), label=\"total loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6e37d8-7cc9-4249-805f-e4619c29d9e5",
   "metadata": {},
   "source": [
    "### Joint alternate training (regularized)\n",
    "\n",
    "**with `jinns.solve_alternate()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7311d0d-fe8e-4a38-8edc-16af10b8cbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_GLV_JointEstimation_Alternate import soft_thresholding_additive_update\n",
    "\n",
    "soft_thresholding_additive_update_g = partial(\n",
    "    soft_thresholding_additive_update, param=\"g\"\n",
    ")\n",
    "soft_thresholding_additive_update_i = partial(\n",
    "    soft_thresholding_additive_update, param=\"i\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5e8136-f52c-4eec-82ab-8a66ab4fb5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solver for each set of parameters\n",
    "optimizers = jinns.parameters.Params(\n",
    "    nn_params=optax.adamw(learning_rate=1e-3),\n",
    "    eq_params={\n",
    "        \"g\": optax.chain(\n",
    "            optax.adam(learning_rate=1e-3),\n",
    "            soft_thresholding_additive_update_g(learning_rate=1e-3, l1reg=1e-5),\n",
    "        ),\n",
    "        \"i\": optax.chain(\n",
    "            optax.adam(learning_rate=1e-6),\n",
    "            soft_thresholding_additive_update_i(learning_rate=1e-6, l1reg=1e-5),\n",
    "        ),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec9a09c-4eef-46b5-a355-f61ccb4011f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    params_alternate_regularized,\n",
    "    total_loss_values,\n",
    "    loss_by_term_dict,\n",
    "    _,\n",
    "    _,\n",
    "    _,\n",
    "    stored_params,\n",
    "    stored_loss_weight_terms,\n",
    "    _,\n",
    "    _,\n",
    ") = jinns.solve_alternate(\n",
    "    n_iter=n_iter,\n",
    "    n_iter_by_solver=n_iter_by_solver,\n",
    "    init_params=init_params,#pretrain_params,\n",
    "    data=train_data,\n",
    "    loss=loss,  # take the complete loss!\n",
    "    optimizers=optimizers,\n",
    "    verbose=True,\n",
    "    obs_data=obs_data,\n",
    "    key=subkey,\n",
    "    print_loss_every=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f0e11f-f143-486b-b464-ddbffbb5fca4",
   "metadata": {},
   "source": [
    "**Results**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fa60d9-2c17-423d-80f2-46ce7433e1ba",
   "metadata": {},
   "source": [
    "*Plot loss values*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bf5045-f6fc-41b3-bc85-e28dcc05e55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for loss_name, loss_values in loss_by_term_dict.items():\n",
    "    # if loss_name == \"observations\":\n",
    "    plt.plot(jnp.log10(loss_values), label=loss_name)\n",
    "plt.plot(jnp.log10(total_loss_values), label=\"total loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62bf259-79d9-44a5-8562-df591655a99d",
   "metadata": {},
   "source": [
    "### Vanilla joint training\n",
    "\n",
    "**with `jinns.solve()`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a461e7f5-e14b-4bb7-a011-56f51c7da141",
   "metadata": {},
   "source": [
    "**Loss for `jinns.solve()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb3b7ed-b3f7-469d-9ee5-7a126a2c5562",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = jinns.loss.LossODE(\n",
    "    u=u,\n",
    "    loss_weights=jinns.loss.LossWeightsODE(\n",
    "        dyn_loss=1.0,\n",
    "        initial_condition=1.0,\n",
    "        observations=1.0,\n",
    "    ),\n",
    "    dynamic_loss=dynamic_loss,\n",
    "    initial_condition=(float(tmin), jnp.log(y0)),\n",
    "    derivative_keys=derivative_keys,\n",
    "    params=init_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebc19c4-e066-4c8a-9bd5-c35d616fec72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from utils_GLV_JointEstimation_Alternate import update_and_project\n",
    "\n",
    "update_and_project_g = partial(update_and_project, param=\"g\")\n",
    "update_and_project_i = partial(update_and_project, param=\"i\")\n",
    "\n",
    "param_labels = jinns.parameters.Params(\n",
    "    nn_params=\"nn_adam\",  # in this simple example we will use adamw for the nu\n",
    "    eq_params={\n",
    "        \"g\": \"g_adam\",\n",
    "        \"i\": \"i_adam\",\n",
    "    },  # in this simple example we will use rprop for theta\n",
    ")\n",
    "\n",
    "tx_nu_and_theta = optax.multi_transform(\n",
    "    {\n",
    "        \"nn_adam\": optax.adamw(learning_rate=1e-3),\n",
    "        \"g_adam\": optax.chain(\n",
    "            optax.adam(learning_rate=1e-3),\n",
    "            update_and_project_g(),\n",
    "        ),\n",
    "        \"i_adam\": optax.chain(\n",
    "            optax.adam(learning_rate=1e-6),\n",
    "            update_and_project_i(),\n",
    "        ),\n",
    "    },  # all the keys in this dict must be found as leaves in the PyTree param_labels\n",
    "    param_labels,\n",
    ")\n",
    "\n",
    "key, subkey = jax.random.split(key, 2)\n",
    "n_iter_nu_and_theta = (n_iter * sum(jax.tree.leaves(n_iter_by_solver))) // (\n",
    "    len(jax.tree.leaves(n_iter_by_solver))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1772a1a7-57b2-46eb-bc56-bd3b430bd83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_vanilla, total_loss_values, loss_by_term_dict, _, _, _, _, _, _, _, _, _ = (\n",
    "    jinns.solve(\n",
    "        init_params=pretrain_params,\n",
    "        data=train_data,\n",
    "        optimizer=tx_nu_and_theta,\n",
    "        loss=loss,\n",
    "        n_iter=n_iter_nu_and_theta,\n",
    "        obs_data=obs_data,\n",
    "        print_loss_every=10000,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e756c8-5b56-4f54-9b19-5629796d55b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for loss_name, loss_values in loss_by_term_dict.items():\n",
    "    # if loss_name == \"observations\":\n",
    "    plt.plot(jnp.log10(loss_values), label=loss_name)\n",
    "plt.plot(jnp.log10(total_loss_values), label=\"total loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95342f6e-f9b0-4190-b578-db2481914c42",
   "metadata": {},
   "source": [
    "### Vanilla joint training (regularized)\n",
    "\n",
    "**with `jinns.solve()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0999da6b-15da-4be0-8873-ec831b7e0c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from utils_GLV_JointEstimation_Alternate import update_and_project\n",
    "\n",
    "update_and_project_g = partial(update_and_project, param=\"g\")\n",
    "update_and_project_i = partial(update_and_project, param=\"i\")\n",
    "\n",
    "param_labels = jinns.parameters.Params(\n",
    "    nn_params=\"nn_adam\",  # in this simple example we will use adamw for the nu\n",
    "    eq_params={\n",
    "        \"g\": \"g_prox\",\n",
    "        \"i\": \"i_prox\",\n",
    "    },  # in this simple example we will use rprop for theta\n",
    ")\n",
    "\n",
    "tx_nu_and_theta = optax.multi_transform(\n",
    "    {\n",
    "        \"nn_adam\": optax.adamw(learning_rate=1e-3),\n",
    "        \"g_prox\": optax.chain(\n",
    "            optax.adam(learning_rate=1e-3),\n",
    "            soft_thresholding_additive_update_g(learning_rate=1e-3, l1reg=1e-5),\n",
    "        ),\n",
    "        \"i_prox\": optax.chain(\n",
    "            optax.adam(learning_rate=1e-6),\n",
    "            soft_thresholding_additive_update_i(learning_rate=1e-6, l1reg=1e-5),\n",
    "        ),\n",
    "    },  # all the keys in this dict must be found as leaves in the PyTree param_labels\n",
    "    param_labels,\n",
    ")\n",
    "\n",
    "key, subkey = jax.random.split(key, 2)\n",
    "n_iter_nu_and_theta = (n_iter * sum(jax.tree.leaves(n_iter_by_solver))) // (\n",
    "    len(jax.tree.leaves(n_iter_by_solver))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581810a6-55e3-4891-85aa-850f0bd2ea20",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    params_vanilla_regularized,\n",
    "    total_loss_values,\n",
    "    loss_by_term_dict,\n",
    "    _,\n",
    "    _,\n",
    "    _,\n",
    "    _,\n",
    "    _,\n",
    "    _,\n",
    "    _,\n",
    "    _,\n",
    "    _,\n",
    ") = jinns.solve(\n",
    "    init_params=pretrain_params,\n",
    "    data=train_data,\n",
    "    optimizer=tx_nu_and_theta,\n",
    "    loss=loss,\n",
    "    n_iter=n_iter_nu_and_theta,\n",
    "    obs_data=obs_data,\n",
    "    print_loss_every=10000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2139096e-48dd-43ea-ac84-11cbff6dddf4",
   "metadata": {},
   "source": [
    "*Errors on parameters*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4ecca7-ef6f-4717-925f-5bbc3b4d0a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "for loss_name, loss_values in loss_by_term_dict.items():\n",
    "    # if loss_name == \"observations\":\n",
    "    plt.plot(jnp.log10(loss_values), label=loss_name)\n",
    "plt.plot(jnp.log10(total_loss_values), label=\"total loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1bc961-653c-4771-b0e6-134cd57e3bb0",
   "metadata": {},
   "source": [
    "## Comparison of the approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3d21d2-3ebd-4030-aaf1-67f3f37c90ba",
   "metadata": {},
   "source": [
    "*Error on parameters*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fca1e6e-41b3-4d8e-8e1d-8d88314911d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import fields\n",
    "\n",
    "\n",
    "def errors_on_parameters(true, estimated):\n",
    "    return_dict = {}\n",
    "    for f in fields(estimated.eq_params):\n",
    "        p = getattr(estimated.eq_params, f.name)\n",
    "        p_true = getattr(true, f.name)\n",
    "\n",
    "        rmse = jnp.mean(\n",
    "            jax.tree.reduce(  # the tree.reduce does not have an effect on carrying_capacities and growth_rates\n",
    "                lambda a, b: (a + b) / 2,\n",
    "                jax.tree.map(lambda p1, p2: (p1 - p2) ** 2, p, p_true),\n",
    "                0,\n",
    "            )\n",
    "        )\n",
    "        return_dict[f.name] = rmse.item()\n",
    "    return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8098d5f6-2be4-4ad0-99f9-ce28c13251ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jinns.parameters import EqParams\n",
    "\n",
    "true_params = EqParams({\"g\": growth_rates, \"i\": interactions})\n",
    "\n",
    "rmse_parameters_alternate = errors_on_parameters(true_params, params_alternate)\n",
    "rmse_parameters_alternate_regularized = errors_on_parameters(\n",
    "    true_params, params_alternate_regularized\n",
    ")\n",
    "rmse_parameters_vanilla = errors_on_parameters(true_params, params_vanilla)\n",
    "rmse_parameters_vanilla_regularized = errors_on_parameters(\n",
    "    true_params, params_vanilla_regularized\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337c330f-4638-4009-9b6f-e11fafb64199",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_vanilla.eq_params.g, params_alternate_regularized.eq_params.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f18b5a6-86a6-4fb8-acaa-5a0abc52742c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{rmse_parameters_alternate=}\")\n",
    "print(f\"{rmse_parameters_alternate_regularized=}\")\n",
    "print(f\"{rmse_parameters_vanilla=}\")\n",
    "print(f\"{rmse_parameters_vanilla_regularized=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4b2cc3-d202-488a-b358-447d74c9b1d3",
   "metadata": {},
   "source": [
    "*Errors on the curves*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298b1007-1c6f-4d4e-8beb-886415595e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_on_the_curves(y, pinn_estimated_function, params_estimated):\n",
    "    \"\"\"\n",
    "    y: true curves\n",
    "    \"\"\"\n",
    "    # Profiled error: RMSE error between the true curve and the PINN estimation\n",
    "    rmse_p = jnp.sum(\n",
    "        jax.vmap(lambda y, y_: jnp.mean((y - y_) ** 2), (1, 1))(\n",
    "            y, pinn_estimated_function(ts)\n",
    "        )\n",
    "    )\n",
    "    # print(\"RMSE profiled error\", rmse_p)\n",
    "\n",
    "    # Reconstructed error: RMSE error between the true curve and the ground truth solver solution run\n",
    "    # with the system parameters estimated by jinns.solve_alternate()\n",
    "    rec_sol_log = solve_ivp(\n",
    "        fun=GLV_RHS,\n",
    "        y0=y0_log,\n",
    "        t_span=(0, Tmax),\n",
    "        t_eval=ts * Tmax,\n",
    "        args=(\n",
    "            (\n",
    "                params_estimated.eq_params.g,\n",
    "                params_estimated.eq_params.i,\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "    rec_y = np.exp(rec_sol_log.y).T\n",
    "    rmse_r = jnp.sum(jax.vmap(lambda y, y_: jnp.mean((y - y_) ** 2), (1, 1))(y, rec_y))\n",
    "    # print(\"RMSE profiled error\", rmse_r)\n",
    "\n",
    "    rmse_curves = {\"profiled\": rmse_p.item(), \"reconstructed\": rmse_r.item()}\n",
    "    return rmse_curves, rec_sol_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d5a027-9eb5-471f-86c2-e0d1e6caaa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_u_est_alternate = jax.vmap(lambda t_x: jnp.exp(u(t_x, params_alternate)))\n",
    "trained_u_est_alternate_regularized = jax.vmap(\n",
    "    lambda t_x: jnp.exp(u(t_x, params_alternate_regularized))\n",
    ")\n",
    "trained_u_est_vanilla = jax.vmap(lambda t_x: jnp.exp(u(t_x, params_vanilla)))\n",
    "trained_u_est_vanilla_regularized = jax.vmap(\n",
    "    lambda t_x: jnp.exp(u(t_x, params_vanilla_regularized))\n",
    ")\n",
    "\n",
    "\n",
    "rmse_curves_alternate, rec_sol_log_alternate = error_on_the_curves(\n",
    "    y, trained_u_est_alternate, params_alternate\n",
    ")\n",
    "rmse_curves_alternate_regularized, rec_sol_log_alternate_regularized = (\n",
    "    error_on_the_curves(\n",
    "        y, trained_u_est_alternate_regularized, params_alternate_regularized\n",
    "    )\n",
    ")\n",
    "rmse_curves_vanilla, rec_sol_log_vanilla = error_on_the_curves(\n",
    "    y, trained_u_est_vanilla, params_vanilla\n",
    ")\n",
    "rmse_curves_vanilla_regularized, rec_sol_log_vanilla_regularized = error_on_the_curves(\n",
    "    y, trained_u_est_vanilla_regularized, params_vanilla_regularized\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f419181-738f-4e71-aadf-ccfcb6ba2c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{rmse_curves_alternate=}\")\n",
    "print(f\"{rmse_curves_alternate_regularized=}\")\n",
    "print(f\"{rmse_curves_vanilla=}\")\n",
    "print(f\"{rmse_curves_vanilla_regularized=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edf7585-e3bc-4071-a723-1f6f2a901678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "\n",
    "\n",
    "def plot_curves(title, rec_sol_log, pinn_estimated_function):\n",
    "    plt.plot(ts * Tmax, y)\n",
    "    plt.gca().set_prop_cycle(None)\n",
    "    if rec_sol_log.status == 0:\n",
    "        rec_y = np.exp(rec_sol_log.y).T\n",
    "        plt.plot(ts * Tmax, rec_y, linestyle=\"dotted\", label=\"Reconstructed\")\n",
    "    else:\n",
    "        print(\n",
    "            \"Warning! Curves from scipy solver with estimated parameters are not plotted because a problem appeared in the resolution\"\n",
    "        )\n",
    "    plt.gca().set_prop_cycle(None)\n",
    "    plt.plot(ts * Tmax, pinn_estimated_function(ts), linestyle=\"--\", label=\"PINN estimated\")\n",
    "    plt.gca().set_prop_cycle(None)\n",
    "    plt.plot(t_subsample * Tmax, jnp.exp(log_obs_subsample_noisy), 'x')\n",
    "    plt.title(title)\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], color=\"k\", label=\"True\"),\n",
    "        Line2D([0], [0], color=\"k\", linestyle=\"dotted\", label=\"Reconstructed\"),\n",
    "        Line2D([0], [0], color=\"k\", linestyle=\"--\", label=\"PINN estimated\"),\n",
    "        Line2D([0], [0], color=\"k\", marker=\"x\", linestyle=\"none\", label=\"Observations\")\n",
    "    ]\n",
    "    # Create the figure\n",
    "    plt.legend(handles=legend_elements, loc=\"upper left\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f06ea8-f5af-44ae-a408-5d6b0dca442e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(\n",
    "    \"u(t,x) after alternate training\", rec_sol_log_alternate, trained_u_est_alternate\n",
    ")\n",
    "plot_curves(\n",
    "    \"u(t,x) after alternate regularized training\",\n",
    "    rec_sol_log_alternate_regularized,\n",
    "    trained_u_est_alternate_regularized,\n",
    ")\n",
    "plot_curves(\"u(t,x) after vanilla training\", rec_sol_log_vanilla, trained_u_est_vanilla)\n",
    "plot_curves(\n",
    "    \"u(t,x) after vanilla regularized training\",\n",
    "    rec_sol_log_vanilla_regularized,\n",
    "    trained_u_est_vanilla_regularized,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab7fde7-722a-4315-9ef6-8689fbc609a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
